{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from w2_opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from w2_opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    # Simple gradient descend\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        parameters[f\"W{l}\"] = parameters[f\"W{l}\"] - learning_rate * grads['dW' + str(l)]\n",
    "        parameters[f\"b{l}\"] = parameters[f\"b{l}\"] - learning_rate * grads['db' + str(l)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    np.random.seed(seed)           \n",
    "    m = X.shape[1]               \n",
    "    mini_batches = []\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    \n",
    "    inc = mini_batch_size\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        \n",
    "        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    if m % mini_batch_size != 0:\n",
    "        \n",
    "        mini_batch_X = shuffled_X[:,(k+1)*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:,(k+1)*mini_batch_size:]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def initialize_velocity(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        v[f\"dW{l}\"] = np.zeros((parameters[f\"W{l}\"].shape[0], parameters[f\"W{l}\"].shape[1]))\n",
    "        v[f\"db{l}\"] = np.zeros((parameters[f\"b{l}\"].shape[0], parameters[f\"b{l}\"].shape[1]))\n",
    "        \n",
    "        \n",
    "    return v\n",
    "\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        v[f\"dW{l}\"] = beta * v[f\"dW{l}\"] + (1-beta) * grads[f\"dW{l}\"]\n",
    "        v[f\"db{l}\"] = beta * v[f\"db{l}\"] + (1-beta) * grads[f\"db{l}\"]\n",
    "        parameters[f\"W{l}\"] = parameters[f\"W{l}\"] - learning_rate * v[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] = parameters[f\"b{l}\"] - learning_rate * v[f\"db{l}\"]\n",
    "        \n",
    "    return parameters, v\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        v[f\"dW{l}\"] = np.zeros((parameters[f\"W{l}\"].shape[0],parameters[f\"W{l}\"].shape[1]))\n",
    "        v[f\"db{l}\"] = np.zeros((parameters[f\"b{l}\"].shape[0],parameters[f\"b{l}\"].shape[1]))\n",
    "        s[f\"dW{l}\"] = np.zeros((parameters[f\"W{l}\"].shape[0],parameters[f\"W{l}\"].shape[1]))\n",
    "        s[f\"db{l}\"] = np.zeros((parameters[f\"b{l}\"].shape[0],parameters[f\"b{l}\"].shape[1]))\n",
    "        \n",
    "    return v, s\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: update_parameters_with_adam\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    t -- Adam variable, counts the number of taken steps\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(1, L + 1):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        # (approx. 2 lines)\n",
    "        # v[\"dW\" + str(l)] = ...\n",
    "        # v[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        v[f\"dW{l}\"] = beta1 * v[f\"dW{l}\"] + (1-beta1) * grads[f\"dW{l}\"]\n",
    "        v[f\"db{l}\"] = beta1 * v[f\"db{l}\"] + (1-beta1) * grads[f\"db{l}\"]\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        # (approx. 2 lines)\n",
    "        # v_corrected[\"dW\" + str(l)] = ...\n",
    "        # v_corrected[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        v_corrected[f\"dW{l}\"] = v[f\"dW{l}\"] / (1 - np.power(beta1, t) )\n",
    "        v_corrected[f\"db{l}\"] = v[f\"db{l}\"] / (1 - np.power(beta1, t) )\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        #(approx. 2 lines)\n",
    "        # s[\"dW\" + str(l)] = ...\n",
    "        # s[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        s[f\"dW{l}\"] = beta2 * s[f\"dW{l}\"] + (1 - beta2) * grads[f\"dW{l}\"] * grads[f\"dW{l}\"]\n",
    "        s[f\"db{l}\"] = beta2 * s[f\"db{l}\"] + (1 - beta2) * grads[f\"db{l}\"] * grads[f\"db{l}\"]\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        # (approx. 2 lines)\n",
    "        # s_corrected[\"dW\" + str(l)] = ...\n",
    "        # s_corrected[\"db\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        s_corrected[f\"dW{l}\"] = s[f\"dW{l}\"] / (1 - np.power(beta2, t) )\n",
    "        s_corrected[f\"db{l}\"] = s[f\"db{l}\"] / (1 - np.power(beta2, t) )\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        # (approx. 2 lines)\n",
    "        # parameters[\"W\" + str(l)] = ...\n",
    "        # parameters[\"b\" + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters[f\"W{l}\"] = parameters[f\"W{l}\"] - learning_rate * v_corrected[f\"dW{l}\"] / np.sqrt(s_corrected[f\"dW{l}\"] + epsilon)\n",
    "        parameters[f\"b{l}\"] = parameters[f\"b{l}\"] - learning_rate * v_corrected[f\"db{l}\"] / np.sqrt(s_corrected[f\"db{l}\"] + epsilon)\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
